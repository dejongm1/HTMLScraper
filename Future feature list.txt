Future feature list

getRecords()
	avoid blacklisting ---- already trying crawl slowly, rotate User-agents
	    randomize sleep time
        use Connection.proxy() for IP masking?

	    generate full list of result page links, randomize then get docs and store in HashMap
	        -re-sort to search for any records have already been scraped
	        -remove it and subsequent records
	    parse thses docs for details links and store in hashmap
	    find some random hrefs to hashmap as well (use jSoup.listLinks())
	    randomize list again
	    go through map, scraping details docs and ignoring others
	    sort by arrest date in spreadsheet. That will make it easier to find already parsed records later on

	On failed getHtmlAsDoc(baseDoc) getHtmlAsDoc(resultsPage), retry 3 times with different user-agent
	On failed getHtmlAsDoc(recordDetail) retry once, keeping track of failed pages
		- save to list or retry immediately?
	After so many failed getHtmlAsDoc()s stop trying
	
	Send e-mail when finished
		- store addresses and details in props
		- maybe separate e-mail.properties
	
	Don't override spreadsheet each run
	One sheet per state, one book per app execution
	
	Only save to spreadsheet if it's not found already
		- running every week will stop if we reach previous records
		- need to save spreadsheet if we do it this way
	
	Start where we left off? Not sure how I would do that
		- keep track of the page we're on? we would only retry a certain number of records
		- output more info per record to spreadsheet. Read the spreadsheet in first, then start there
		
	Keep properties file separate from jar?
		-use it to tweak things like site(s), columns to export, flags, etc
	



Constants class for messages
Search for specific word(s)
	- variations of those words

Exclude special characters (and numbers?)
Add help documentation
Result exclusion file for frequent words
	- flag for excluding common words (the, a, and, of, in, etc)
	- flag for including numbers
	- add to/remove from list


Traverse a sites pages
	- specified level deep
	- complete site (possible by domain?)
	- find all <a href=>

If website can't be reached with http://, try https://

